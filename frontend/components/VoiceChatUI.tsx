"use client";

import { useEffect, useState, useRef } from 'react';
import io, { Socket } from 'socket.io-client';
import animationData from '../public/animation.json'; // public ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ãƒ‘ã‚¹ã«å¤‰æ›´
import Lottie from 'react-lottie';

// ãƒã‚¤ã‚¯ã®çŠ¶æ…‹ã‚’è¡¨ã™å‹
type MicState = 'idle' | 'recording' | 'playing';

// ç’°å¢ƒå¤‰æ•°ã‚’å–å¾— (ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã§åˆ©ç”¨å¯èƒ½)
const nestJsWsUrl = process.env.NEXT_PUBLIC_NESTJS_WS_URL || 'ws://127.0.0.1:3001/voice-chat'; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
const fastApiWsUrlBase = process.env.NEXT_PUBLIC_FASTAPI_WS_URL || 'ws://127.0.0.1:5000/ws/'; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š

export default function VoiceChatUI() {
  const [micState, setMicState] = useState<MicState>('idle'); // 'idle', 'recording', 'playing'
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  const lottieRef = useRef<any>(null);
  const [, setSocket] = useState<Socket | null>(null); // NestJS Socket (ç¾çŠ¶ç¶­æŒ)
  const [fastAPIWebSocket, setFastAPIWebSocket] = useState<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);

  useEffect(() => {
    // NestJS WebSocket ã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶š (ç¢ºèªç”¨ - å¤‰æ›´ãªã—)
    console.log("Using NestJS WS URL:", nestJsWsUrl);
    console.log("Using FastAPI WS Base URL:", fastApiWsUrlBase);
    // ç’°å¢ƒå¤‰æ•°ã®URLãŒ `ws://host:port/namespace` å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’æƒ³å®š
    // Socket.IOã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ http/https ã§æ¥ç¶šã‚’é–‹å§‹ã™ã‚‹
    const nestHttpUrl = nestJsWsUrl.replace(/^ws/, 'http');
    const nestSocket = io(nestHttpUrl); // URLã«åå‰ç©ºé–“ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã¨ä»®å®š
    setSocket(nestSocket);

    // FastAPI WebSocket ã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶š
    const clientId = generateClientId();
    const fastApiWsUrl = `${fastApiWsUrlBase}${clientId}`;
    const ws = new WebSocket(fastApiWsUrl);
    setFastAPIWebSocket(ws);

    ws.onopen = () => {
      console.log('FastAPI WebSocket connection opened:', clientId);
    };

    ws.onmessage = (event) => {
      console.log('Message from FastAPI server:', event.data);
      // FastAPIã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’å—ä¿¡ã—ãŸã‚‰èª­ã¿ä¸Šã’é–‹å§‹
      if (typeof event.data === 'string') {
          speak(event.data);
      } else if (event.data instanceof Blob) {
          // Blobãƒ‡ãƒ¼ã‚¿ï¼ˆéŸ³å£°ãªã©ï¼‰ã‚’å—ä¿¡ã—ãŸå ´åˆã®å‡¦ç†ï¼ˆå¿…è¦ãªã‚‰ï¼‰
          console.log("Received audio data from FastAPI");
          // ã“ã“ã§Blobãƒ‡ãƒ¼ã‚¿ã‚’å†ç”Ÿã™ã‚‹ãªã©ã®å‡¦ç†ã‚’è¿½åŠ ã§ãã¾ã™
          // ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ï¼ˆéŸ³å£°ï¼‰ãŒå±Šã„ãŸå ´åˆ
          setMicState('playing'); // å†ç”Ÿä¸­çŠ¶æ…‹ã«ã™ã‚‹
          const audioUrl = URL.createObjectURL(event.data);
          const audio = new Audio(audioUrl);
          audio.play();
          audio.onended = () => setMicState('idle'); // å†ç”Ÿå®Œäº†ã§idleã«æˆ»ã™
          // ä¾‹: const audioUrl = URL.createObjectURL(event.data);
          //     const audio = new Audio(audioUrl);
          //     audio.play();
          //     setMicState('playing'); // å†ç”Ÿä¸­çŠ¶æ…‹ã«ã™ã‚‹
          //     audio.onended = () => setMicState('idle'); // å†ç”Ÿå®Œäº†ã§idleã«æˆ»ã™
      }
    };

    ws.onerror = (error) => {
      console.error('FastAPI WebSocket error:', error);
      setMicState('idle'); // ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯idleçŠ¶æ…‹ã«æˆ»ã™
    };

    ws.onclose = () => {
        console.log('FastAPI WebSocket connection closed.');
        setMicState('idle'); // åˆ‡æ–­æ™‚ã‚‚idleçŠ¶æ…‹ã«æˆ»ã™
    }

    return () => {
      nestSocket.disconnect();
      ws.close();
      stopBrowserRecording(); // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç ´æ£„æ™‚ã«éŒ²éŸ³åœæ­¢
    };
  }, []); // ä¾å­˜é…åˆ—ã¯ç©ºã®ã¾ã¾

  const generateClientId = () => {
    return "client_" + Math.random().toString(36).substring(2, 15);
  };

  const speak = (text: string) => {
    setMicState('playing'); // èª­ã¿ä¸Šã’é–‹å§‹æ™‚ã«çŠ¶æ…‹ã‚’ playing ã«å¤‰æ›´
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'ja-JP';
    utterance.onstart = () => {
        console.log("Speech synthesis started");
        setMicState('playing'); //å¿µã®ãŸã‚onstartã§ã‚‚è¨­å®š
    };
    utterance.onend = () => {
      console.log("Speech synthesis ended");
      setMicState('idle'); // èª­ã¿ä¸Šã’å®Œäº†æ™‚ã«çŠ¶æ…‹ã‚’ idle ã«å¤‰æ›´
    };
    utterance.onerror = (e) => {
        console.error("Speech synthesis error:", e);
        setMicState('idle'); // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚idleã«æˆ»ã™
    }
    speechSynthesis.speak(utterance);
  };

  // ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®éŒ²éŸ³ã‚’é–‹å§‹ã™ã‚‹é–¢æ•°
  const startBrowserRecording = async () => {
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      console.error('getUserMedia not supported on your browser!');
      alert('ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°å…¥åŠ›ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚');
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaStreamRef.current = stream;
      const context = new AudioContext();
      audioContextRef.current = context;
      const source = context.createMediaStreamSource(stream);
      const processor = context.createScriptProcessor(4096, 1, 1); // ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã€å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã€å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°
      processorRef.current = processor;

      processor.onaudioprocess = (e) => {
        if (micState !== 'recording' || !fastAPIWebSocket || fastAPIWebSocket.readyState !== WebSocket.OPEN) {
          return;
        }
        // Float32Array ã‚’ Int16Array ã«å¤‰æ›ã—ã¦é€ä¿¡
        const inputData = e.inputBuffer.getChannelData(0);
        const output = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
            output[i] = Math.max(-1, Math.min(1, inputData[i])) * 0x7FFF; // 16ãƒ“ãƒƒãƒˆæ•´æ•°ã«å¤‰æ›
        }
        // console.log("Sending audio data chunk..."); // ãƒ­ã‚°ãŒå¤šã„ã®ã§ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        console.debug("é€ä¿¡å‰ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆArrayBufferã‚µã‚¤ã‚ºï¼‰:", output.buffer.byteLength);
        fastAPIWebSocket.send(output.buffer);
      };

      source.connect(processor);
      processor.connect(context.destination); // ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’æœ€çµ‚å‡ºåŠ›ã«æ¥ç¶šï¼ˆéŸ³ã‚’èãã‚ã‘ã§ã¯ãªã„ãŒæ¥ç¶šã¯å¿…è¦ï¼‰

      console.log("Browser recording started");

    } catch (err) {
      console.error('Error starting browser recording:', err);
      alert('ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ãŒå¿…è¦ã§ã™ã€‚');
      setMicState('idle'); // ã‚¨ãƒ©ãƒ¼æ™‚ã¯idleçŠ¶æ…‹ã«æˆ»ã™
    }
  };

  // ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®éŒ²éŸ³ã‚’åœæ­¢ã™ã‚‹é–¢æ•°
  const stopBrowserRecording = () => {
    console.log("Stopping browser recording...");
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current.onaudioprocess = null; // ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼è§£é™¤
      processorRef.current = null;
    }
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        // AudioContextã‚’é–‰ã˜ã‚‹å‰ã«å°‘ã—å¾…ã¤ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰
        setTimeout(() => {
            audioContextRef.current?.close().catch(e => console.error("Error closing AudioContext:", e));
            audioContextRef.current = null;
            console.log("AudioContext closed.");
        }, 100); // 100mså¾…ã¤
    }
  };

  // ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©
  const handleMicClick = () => {
    if (micState === 'idle') {
      // å¾…æ©ŸçŠ¶æ…‹ -> éŒ²éŸ³é–‹å§‹
      setMicState('recording');
      startBrowserRecording(); // ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®éŒ²éŸ³é–‹å§‹
      // FastAPI ã‚µãƒ¼ãƒãƒ¼ã«éŒ²éŸ³é–‹å§‹ã‚’é€šçŸ¥ (å¿…è¦ã§ã‚ã‚Œã°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹)
      if (fastAPIWebSocket && fastAPIWebSocket.readyState === WebSocket.OPEN) {
          console.log("Sending start_recording signal to FastAPI");
          // å¿…è¦ã«å¿œã˜ã¦é–‹å§‹ã®åˆå›³ã‚’é€ã‚‹ï¼ˆä¾‹: JSONãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
          fastAPIWebSocket.send(JSON.stringify({ type: "start_recording" }));
      }

    } else if (micState === 'recording') {
      // éŒ²éŸ³çŠ¶æ…‹ -> èª­ã¿ä¸Šã’æº–å‚™/å¾…æ©ŸçŠ¶æ…‹ã¸
      setMicState('idle'); // ã¾ãšidleã«æˆ»ã—ã€FastAPIã‹ã‚‰ã®å¿œç­”ã‚’å¾…ã¤
      stopBrowserRecording(); // ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®éŒ²éŸ³åœæ­¢
       // FastAPI ã‚µãƒ¼ãƒãƒ¼ã«éŒ²éŸ³åœæ­¢ã‚’é€šçŸ¥ï¼ˆFastAPIå´ãŒéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®çµ‚ç«¯ã‚’æ¤œçŸ¥ã™ã‚‹ãªã‚‰ä¸è¦ãªå ´åˆã‚‚ï¼‰
      if (fastAPIWebSocket && fastAPIWebSocket.readyState === WebSocket.OPEN) {
          console.log("Sending stop_recording signal to FastAPI");
          // fastAPIWebSocket.send(JSON.stringify({ type: "stop_recording" }));
          // ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚’é€ã‚‹ã“ã¨ã§çµ‚ç«¯ã‚’ç¤ºã™å ´åˆã‚‚ã‚ã‚‹
          fastAPIWebSocket.send(new ArrayBuffer(0));
      }
    } else if (micState === 'playing') {
        // å†ç”Ÿä¸­ -> ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã¸ï¼ˆå†ç”Ÿåœæ­¢ï¼‰
        speechSynthesis.cancel(); // ç¾åœ¨ã®èª­ã¿ä¸Šã’ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        setMicState('idle');
        console.log("Speech synthesis cancelled by user.");
    }
  };

  // Lottieã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
  const defaultOptions = {
    loop: true,
    autoplay: false, // micStateã«å¿œã˜ã¦åˆ¶å¾¡ã™ã‚‹ã®ã§autoplayã¯false
    animationData: animationData,
    rendererSettings: {
      preserveAspectRatio: 'xMidYMid slice'
    }
  };

  // ãƒœã‚¿ãƒ³ã®è‰²ã‚’æ±ºå®š
  const getButtonColor = () => {
    switch (micState) {
      case 'recording':
        return 'red';
      case 'playing':
        return 'green';
      case 'idle':
      default:
        return 'blue';
    }
  };

  // è¡¨ç¤ºã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ±ºå®š
  const getStatusMessage = () => {
    switch (micState) {
      case 'recording':
        return 'ãŠã¯ãªã—ã¡ã‚…ã†â€¦';
      case 'playing':
        return 'ãŠã¸ã‚“ã˜ã¡ã‚…ã†â€¦';
      case 'idle':
      default:
        return ''; // idleçŠ¶æ…‹ã§ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—
    }
  };

  return (
    <div style={{ textAlign: 'center', display: 'flex', flexDirection: 'column', alignItems: 'center' }}>
      <div style={{ width: '200px', height: '200px', marginBottom: '20px' }}>
        <Lottie
          options={defaultOptions}
          height={200}
          width={200}
          isStopped={micState !== 'playing'} // playing çŠ¶æ…‹ã§ãªã„ã¨ãã¯åœæ­¢
          isPaused={micState !== 'playing'}  // playing çŠ¶æ…‹ã§ãªã„ã¨ãã¯ä¸€æ™‚åœæ­¢
          ref={lottieRef}
        />
      </div>
      <button
        style={{
          width: '100px',
          height: '100px',
          borderRadius: '50%',
          backgroundColor: getButtonColor(), // çŠ¶æ…‹ã«å¿œã˜ã¦è‰²ã‚’å¤‰æ›´
          border: 'none',
          color: 'white', // ã‚¢ã‚¤ã‚³ãƒ³ãŒè¦‹ã‚„ã™ã„ã‚ˆã†ã«ç™½æ–‡å­—ã«
          fontSize: '50px',
          cursor: 'pointer', // ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ã‚’ç¤ºã™ã‚«ãƒ¼ã‚½ãƒ«
          display: 'flex',
          justifyContent: 'center',
          alignItems: 'center',
          transition: 'background-color 0.3s ease', // è‰²å¤‰åŒ–ã‚’æ»‘ã‚‰ã‹ã«
        }}
        onClick={handleMicClick} // ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆã«å¤‰æ›´
      >
        ğŸ¤
      </button>
      <p style={{ marginTop: '15px', height: '20px', color: '#555' }}> {/* ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºé ˜åŸŸ */}
        {getStatusMessage()}
      </p>
    </div>
  );
}